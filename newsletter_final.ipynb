{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newsletter Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import newspaper \n",
    "from newspaper import Article\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Zac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Zac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk imports\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.text import Text\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image imports\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopping the working from slice of dataframe from coming up\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summariser imports\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export imports\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather import\n",
    "from pyowm.owm import OWM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock inports\n",
    "import pandas_datareader as pdr\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Articles\n",
    "\n",
    "websites = ['https://www.theverge.com', 'https://medium.com/tag/technology', 'https://towardsdatascience.com', 'https://python.plainenglish.io/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = ['https://medium.com/tag/technology', 'https://towardsdatascience.com', 'https://python.plainenglish.io/', 'https://www.kdnuggets.com', 'https://www.dataversity.net', 'https://www.ibm.com/blogs/journey-to-ai/', 'https://insidebigdata.com', 'https://www.datarobot.com/blog/', 'https://dataaspirant.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to pull articles from websites:\n",
    "\n",
    "def websites_pull(websites, no_articles):\n",
    "\n",
    "    website = []\n",
    "    title = []\n",
    "    tags = []\n",
    "    body = []\n",
    "    authors = []\n",
    "    top_image = []\n",
    "    keywords = []\n",
    "        \n",
    "    for url in websites:\n",
    "\n",
    "        paper = newspaper.build(url, memoize_articles=False)\n",
    "        \n",
    "        paper_articles = []\n",
    "\n",
    "        print(url)\n",
    "\n",
    "        for article in paper.articles[1:30]:\n",
    "                if '#comments' not in article.url:\n",
    "                    paper_articles.append(article.url)\n",
    "\n",
    "        if len(paper_articles) > no_articles:\n",
    "\n",
    "            for i in tqdm(range(no_articles)):\n",
    "\n",
    "                try:\n",
    "\n",
    "                    article = Article(paper_articles[i], language=\"en\")\n",
    "\n",
    "                    article.download()\n",
    "                    article.parse()\n",
    "\n",
    "                    website.append(article.url)\n",
    "\n",
    "                    title.append(article.title)\n",
    "\n",
    "                    body.append(article.text)\n",
    "\n",
    "                    authors.append(article.authors)\n",
    "\n",
    "                    top_image.append(article.top_image)\n",
    "\n",
    "                    article.nlp()\n",
    "\n",
    "                    keywords.append(article.keywords)\n",
    "\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        else:\n",
    "            for i in tqdm(range(len(paper_articles))):\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        article = Article(paper_articles[i], language=\"en\")\n",
    "\n",
    "                        article.download()\n",
    "                        article.parse()\n",
    "\n",
    "                        website.append(article.url)\n",
    "\n",
    "                        title.append(article.title)\n",
    "\n",
    "                        body.append(article.text)\n",
    "\n",
    "                        authors.append(article.authors)\n",
    "\n",
    "                        top_image.append(article.top_image)\n",
    "\n",
    "                        article.nlp()\n",
    "\n",
    "                        keywords.append(article.keywords) \n",
    "                    \n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "    df = pd.DataFrame({'title':title, 'authors':authors, 'body':body, 'website':website,\n",
    "    'image':top_image, 'keywords':keywords})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://medium.com/tag/technology\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:41<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://towardsdatascience.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:41<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://python.plainenglish.io/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:46<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.kdnuggets.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:11<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.dataversity.net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:16<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ibm.com/blogs/journey-to-ai/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:19<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://insidebigdata.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:13<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.datarobot.com/blog/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:14<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://dataaspirant.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:05<00:00,  1.86s/it]\n"
     ]
    }
   ],
   "source": [
    "# put in a list of websites and the number of articles desired\n",
    "# returns a dataframe of articles\n",
    "\n",
    "df = websites_pull(websites, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df = pd.DataFrame({'title':title, 'authors':authors, 'body':body, 'website':website,\\n 'image':top_image, 'keywords':keywords})\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#¬†to be able to reset data frame without running above code again\n",
    "\n",
    "'''df = pd.DataFrame({'title':title, 'authors':authors, 'body':body, 'website':website,\n",
    " 'image':top_image, 'keywords':keywords})'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting rid of duplicate articles\n",
    "df['title'] = df['title'].drop_duplicates()\n",
    "df = df.dropna()\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the main text - removing \\n\n",
    "df['body'] = df['body'].apply(lambda x:x.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing double spaces\n",
    "df['body'] = df['body'].apply(lambda x:' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning urls\n",
    "df['website']=df['website'].apply(lambda x:x.split('?')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping short articles - sometimes not all the article is scraped\n",
    "df.drop(df[df['body'].str.len() < 1000].index, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# drop articles which start with a number (summariser can't cope with e.g. 7 steps to...) \\n# if number is not 20 - just to keep 2021 and 2022 articles in the mix\\ndf.drop(df[(df['title'].str[0].str.isdigit()) & (df['title'].str[0:2] != '20')].index, inplace=True)\\ndf.reset_index(drop=True, inplace=True)\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# drop articles which start with a number (summariser can't cope with e.g. 7 steps to...) \n",
    "# if number is not 20 - just to keep 2021 and 2022 articles in the mix\n",
    "df.drop(df[(df['title'].str[0].str.isdigit()) & (df['title'].str[0:2] != '20')].index, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bringing back most frequent word pairs found in each article\n",
    "\n",
    "col_final = []\n",
    "\n",
    "for i in range(len(df['body'])):\n",
    "\n",
    "    textfile = df['body'][i]\n",
    "            \n",
    "    tokens = word_tokenize(textfile) # getting tokens\n",
    "\n",
    "    lower_tokens = [token.lower() for token in tokens] # making everying lower case\n",
    "\n",
    "    clean1 = [word for word in lower_tokens if word.isalpha()] # getting rid of numbers\n",
    "\n",
    "    stop_words = stopwords.words('english') # getting stop words\n",
    "    word_m_stop = [word for word in clean1 if not word in stop_words]\n",
    "    final_text = Text(word_m_stop)\n",
    "\n",
    "    col_list = final_text.collocation_list()\n",
    "\n",
    "    col_list = [list(i) for i in col_list]\n",
    "\n",
    "    col_med = []\n",
    "\n",
    "    for i in range(len(col_list)):\n",
    "        sentence = col_list[i]\n",
    "        col_med.append(' '.join(sentence))\n",
    "        \n",
    "    col_final.append(col_med)\n",
    "    \n",
    "df['collocations'] = col_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering for Relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_words_upper = ['Artificial Intelligence',\n",
    " 'Big Data', ''\n",
    " 'Clustering',\n",
    " 'Python',\n",
    " 'Outlier',\n",
    " 'Data Science',\n",
    " 'Data Warehouse',\n",
    " 'Machine Learning',\n",
    " 'Artificial Intelligence',\n",
    " 'Data Analysis',\n",
    " 'Data Engineering',\n",
    " 'Data Visualization',\n",
    " 'Data Wrangling',\n",
    " 'Box Plot',\n",
    " 'Correlation',\n",
    " 'dashboard',\n",
    " 'EDA',\n",
    " 'Histogram',\n",
    " 'Hypothesis',\n",
    " 'Iteration',\n",
    " 'AWS',\n",
    " 'azure',\n",
    " 'Numpy',\n",
    " 'pandas',\n",
    " 'matplotlib',\n",
    " 'seaborn',\n",
    " 'Bayes Theorem',\n",
    " 'Decision Tree',\n",
    " 'Quantile',\n",
    " 'Predictive Modelling',\n",
    " 'Standard Deviation',\n",
    " 'Random Forest',\n",
    " 'boolean',\n",
    " 'Fuzzy Logic',\n",
    " 'Regression',\n",
    " 'Classification',\n",
    " 'Overfit',\n",
    " 'underfit',\n",
    " 'Statistical Significance',\n",
    " 'Variance',\n",
    " 'Deep Learning',\n",
    " 'Feature Selection',\n",
    " 'Supervised Machine Learning',\n",
    " 'Unsupervised Machine Learning',\n",
    " 'Binary Variable',\n",
    " 'Binomial Distribution',\n",
    " 'Computer Vision',\n",
    " 'Confusion Matrix',\n",
    " 'covariance',\n",
    " 'Degrees of Freedom',\n",
    " 'Evaluation Metrics',\n",
    " 'F-Score',\n",
    " 'Hadoop',\n",
    " 'Hyperparameter',\n",
    " 'IQR',\n",
    " 'Keras',\n",
    " 'kNN',\n",
    " 'NoSQL',\n",
    " 'Normal Distribution',\n",
    " 'Normalize',\n",
    " 'One Hot Encoding',\n",
    " 'dummies',\n",
    " 'recall',\n",
    " 'P-Value',\n",
    " 'roc',\n",
    " 'auc',\n",
    " 'Root Mean Squared Error',\n",
    " 'rmse',\n",
    " 'Skewness',\n",
    " 'SMOTE',\n",
    " 'stadardize',\n",
    " 'Standard error',\n",
    " 'TensorFlow',\n",
    " 'Univariate Analysis',\n",
    " 'Z-test',\n",
    " 'Residual',\n",
    " 'Neural Network',\n",
    " 'Autoregression',\n",
    " 'Backpropogation',\n",
    " 'Bagging',\n",
    " 'Bias-Variance Trade-off',\n",
    " 'Boosting',\n",
    " 'Bootstrapping',\n",
    " 'Classification Threshold',\n",
    " 'Convex Function',\n",
    " 'Cosine Similarity',\n",
    " 'Cost Function',\n",
    " 'Cross Entropy',\n",
    " 'Cross Validation',\n",
    " 'DBScan',\n",
    " 'Decision Boundary',\n",
    " 'Dplyr',\n",
    " 'Early Stopping',\n",
    " 'Feature Hashing',\n",
    " 'Gated Recurrent Unit',\n",
    " 'Hidden Markov Model',\n",
    " 'Hierarchical Clustering',\n",
    " 'Holdout Sample',\n",
    " 'Holt-Winters Forecasting',\n",
    " 'Imputation',\n",
    " 'K-Means',\n",
    " 'Kurtosis',\n",
    " 'Lasso',\n",
    " 'Maximum Likelihood Estimation',\n",
    " 'Multivariate Analysis',\n",
    " 'Naive Bayes',\n",
    " 'Polynomial Regression',\n",
    " 'Ridge Regression',\n",
    " 'Rotational Invariance',\n",
    " ' Semi-Supervised Learning',\n",
    " 'Stochastic Gradient Descent',\n",
    " 'SVM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making filtered words lower\n",
    "filter_words = [word.lower() for word in filter_words_upper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating masks - getting boolean arrays for where keywords, collocations or title contain keywords\n",
    "mask = np.array([bool(set(map(str, x)) & set(filter_words)) for x in df['collocations']])\n",
    "mask2 = np.array([bool(set(map(str, x)) & set(filter_words)) for x in df['keywords']])\n",
    "mask3 = np.array([bool(set(map(str, x)) & set(filter_words)) for x in df['title']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering by mask and resetting index\n",
    "df_filtered = df[mask | mask2 | mask3]\n",
    "df_filtered.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†deal with towards data heading !!!\n",
    "# deal with first line issue with body. Does it matter?? !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging articles - beginner, intermediate, advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting lower_case keywords\n",
    "\n",
    "# keywords\n",
    "beginner_upper = ['Artificial Intelligence', 'Big Data', 'Python', 'Outlier', 'Data Science', 'Data Warehouse', 'Machine Learning', 'Artificial Intelligence', 'Data Analysis', 'Data Engineering', 'Data Visualization', 'Data Wrangling', 'Box Plot', 'Correlation', 'dashboard', 'EDA', 'Histogram', 'Hypothesis', 'Iteration', 'AWS', 'azure', 'Numpy', 'pandas', 'matplotlib', 'seaborn']\n",
    "\n",
    "beginner = []\n",
    "\n",
    "for word in beginner_upper:\n",
    "    word = word.lower()\n",
    "    beginner.append(word)\n",
    "\n",
    "medium_upper = ['Clustering','Bayes Theorem', 'Decision Tree', 'Quantile', 'Predictive Modelling', 'Standard Deviation', 'Random Forest', 'boolean', 'Fuzzy Logic', 'Regression', 'Classification', 'Overfit', 'underfit', 'Statistical Significance', 'Variance', 'Deep Learning', 'Feature Selection', 'Supervised Machine Learning', 'Unsupervised Machine Learning', 'Binary Variable', 'Binomial Distribution', 'Computer Vision', 'Confusion Matrix', 'covariance', 'Degrees of Freedom', 'Evaluation Metrics', 'F-Score', 'Hadoop', 'Hyperparameter', 'IQR', 'Keras', 'kNN', 'NoSQL', 'Normal Distribution', 'Normalize', 'One Hot Encoding', 'dummies', 'recall', 'P-Value', 'roc', 'auc', 'Root Mean Squared Error', 'rmse', 'Skewness', 'SMOTE', 'standardize', 'Standard error', 'TensorFlow', 'Univariate Analysis', 'Z-test']\n",
    "\n",
    "medium = []\n",
    "\n",
    "for word in medium_upper:\n",
    "    word = word.lower()\n",
    "    medium.append(word)\n",
    "\n",
    "advanced_upper = ['Residual', 'Neural Network', 'Autoregression', 'Backpropogation', 'Bagging', 'Bias-Variance Trade-off', 'Boosting', 'Bootstrapping', 'Classification Threshold', 'Convex Function', 'Cosine Similarity', 'Cost Function', 'Cross Entropy', 'Cross Validation', 'DBScan', 'Decision Boundary', 'Dplyr', 'Early Stopping', 'Feature Hashing', 'Gated Recurrent Unit', 'Hidden Markov Model', 'Hierarchical Clustering', 'Holdout Sample', 'Holt-Winters Forecasting', 'Imputation', 'K-Means', 'Kurtosis', 'Lasso', 'Maximum Likelihood Estimation', 'Multivariate Analysis', 'Naive Bayes', 'Polynomial Regression', 'Ridge Regression', 'Rotational Invariance', ' Semi-Supervised Learning', 'Stochastic Gradient Descent', 'SVM']\n",
    "\n",
    "advanced = []\n",
    "\n",
    "for word in advanced_upper:\n",
    "    word = word.lower()\n",
    "    advanced.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding columns to dataframe\n",
    "\n",
    "beg = []\n",
    "med = []\n",
    "adv = []\n",
    "\n",
    "\n",
    "for i in range(len(df_filtered['body'])):\n",
    "\n",
    "    beg_count = 0\n",
    "\n",
    "    for word in beginner:\n",
    "        if word in df_filtered['body'][i]:\n",
    "            beg_count +=1\n",
    "\n",
    "    med_count = 0\n",
    "\n",
    "    for word in medium:\n",
    "        if word in df_filtered['body'][i]:\n",
    "            med_count +=1\n",
    "\n",
    "    adv_count = 0\n",
    "\n",
    "    for word in advanced:\n",
    "        if word in df_filtered['body'][i]:\n",
    "            adv_count +=1\n",
    "\n",
    "    total_count = beg_count + med_count + adv_count\n",
    "\n",
    "    if total_count != 0:\n",
    "\n",
    "        beg_percentage = beg_count/total_count\n",
    "        med_percentage = med_count/total_count\n",
    "        adv_percentage = adv_count/total_count\n",
    "\n",
    "        beg.append(beg_percentage)\n",
    "        med.append(med_percentage)\n",
    "        adv.append(adv_percentage)\n",
    "\n",
    "    else:\n",
    "        beg.append(0)\n",
    "        med.append(0)\n",
    "        adv.append(0)\n",
    "\n",
    "df_filtered['percentage_beginner'] = beg\n",
    "df_filtered['percentage_medium'] = med\n",
    "df_filtered['percentage_advanced'] = adv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting back tag column\n",
    "# adjusted advanced threshold\n",
    "\n",
    "tags = []\n",
    "\n",
    "for i in range(len(df_filtered)):\n",
    "    if df_filtered['percentage_advanced'][i] >= 0.3:\n",
    "        tags.append('Advanced')\n",
    "    elif df_filtered['percentage_medium'][i] >= 0.5:\n",
    "        tags.append('Intermediate')\n",
    "    else:\n",
    "        tags.append('Beginner')\n",
    "\n",
    "df_filtered['tag'] = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>body</th>\n",
       "      <th>website</th>\n",
       "      <th>image</th>\n",
       "      <th>keywords</th>\n",
       "      <th>collocations</th>\n",
       "      <th>percentage_beginner</th>\n",
       "      <th>percentage_medium</th>\n",
       "      <th>percentage_advanced</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eight ‚ÄúNo-Code‚Äù Features In Python</td>\n",
       "      <td>[Christopher Tao]</td>\n",
       "      <td>Eight ‚ÄúNo-Code‚Äù Features In Python One of the ...</td>\n",
       "      <td>https://towardsdatascience.com/eight-no-code-f...</td>\n",
       "      <td>https://miro.medium.com/max/1200/1*KTo1ShsunQ4...</td>\n",
       "      <td>[nocode, code, string, m, python, dont, able, ...</td>\n",
       "      <td>[web server, python cli, without writing, writ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Beginner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feature Selection in Scikit-learn</td>\n",
       "      <td>[Zolzaya Luvsandorj]</td>\n",
       "      <td>üìç 3. Feature selection We will look at five di...</td>\n",
       "      <td>https://towardsdatascience.com/feature-selecti...</td>\n",
       "      <td>https://miro.medium.com/max/1200/0*PrHExWptvaF...</td>\n",
       "      <td>[scikitlearn, selected, roc, feature, select, ...</td>\n",
       "      <td>[data roc, roc auc, test data, training data, ...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Intermediate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Guide to Problem-Solving in the Data Industry</td>\n",
       "      <td>[]</td>\n",
       "      <td>1. Occam‚Äôs razor ‚Äî don‚Äôt over-complicate thing...</td>\n",
       "      <td>https://towardsdatascience.com/a-guide-to-prob...</td>\n",
       "      <td>https://miro.medium.com/max/1200/0*p2SX4Leg-oq...</td>\n",
       "      <td>[work, dont, best, actually, excel, problemsol...</td>\n",
       "      <td>[problem solving, group work, needed think, da...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>Beginner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sell Out Sell In Forecasting</td>\n",
       "      <td>[Bartosz Szab≈Çowski]</td>\n",
       "      <td>Create X variables based on cyclic variables, ...</td>\n",
       "      <td>https://towardsdatascience.com/sell-out-sell-i...</td>\n",
       "      <td>https://miro.medium.com/max/1200/1*uoliQIioMkY...</td>\n",
       "      <td>[learning, weeks, model, forecast, week, varia...</td>\n",
       "      <td>[machine learning, image author, multioutput r...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>Intermediate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13 Advanced Python Scripts For Everyday Progra...</td>\n",
       "      <td>[Haider Imtiaz]</td>\n",
       "      <td>1. SpeedTest with Python This advanced script ...</td>\n",
       "      <td>https://python.plainenglish.io/13-advanced-pyt...</td>\n",
       "      <td>https://miro.medium.com/max/1200/1*O5taq5fU3tz...</td>\n",
       "      <td>[scripts, import, code, everyday, script, pyth...</td>\n",
       "      <td>[pip install, recycle bin, window version, hex...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Intermediate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title               authors  \\\n",
       "0                 Eight ‚ÄúNo-Code‚Äù Features In Python     [Christopher Tao]   \n",
       "1                  Feature Selection in Scikit-learn  [Zolzaya Luvsandorj]   \n",
       "2    A Guide to Problem-Solving in the Data Industry                    []   \n",
       "3                       Sell Out Sell In Forecasting  [Bartosz Szab≈Çowski]   \n",
       "4  13 Advanced Python Scripts For Everyday Progra...       [Haider Imtiaz]   \n",
       "\n",
       "                                                body  \\\n",
       "0  Eight ‚ÄúNo-Code‚Äù Features In Python One of the ...   \n",
       "1  üìç 3. Feature selection We will look at five di...   \n",
       "2  1. Occam‚Äôs razor ‚Äî don‚Äôt over-complicate thing...   \n",
       "3  Create X variables based on cyclic variables, ...   \n",
       "4  1. SpeedTest with Python This advanced script ...   \n",
       "\n",
       "                                             website  \\\n",
       "0  https://towardsdatascience.com/eight-no-code-f...   \n",
       "1  https://towardsdatascience.com/feature-selecti...   \n",
       "2  https://towardsdatascience.com/a-guide-to-prob...   \n",
       "3  https://towardsdatascience.com/sell-out-sell-i...   \n",
       "4  https://python.plainenglish.io/13-advanced-pyt...   \n",
       "\n",
       "                                               image  \\\n",
       "0  https://miro.medium.com/max/1200/1*KTo1ShsunQ4...   \n",
       "1  https://miro.medium.com/max/1200/0*PrHExWptvaF...   \n",
       "2  https://miro.medium.com/max/1200/0*p2SX4Leg-oq...   \n",
       "3  https://miro.medium.com/max/1200/1*uoliQIioMkY...   \n",
       "4  https://miro.medium.com/max/1200/1*O5taq5fU3tz...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  [nocode, code, string, m, python, dont, able, ...   \n",
       "1  [scikitlearn, selected, roc, feature, select, ...   \n",
       "2  [work, dont, best, actually, excel, problemsol...   \n",
       "3  [learning, weeks, model, forecast, week, varia...   \n",
       "4  [scripts, import, code, everyday, script, pyth...   \n",
       "\n",
       "                                        collocations  percentage_beginner  \\\n",
       "0  [web server, python cli, without writing, writ...             1.000000   \n",
       "1  [data roc, roc auc, test data, training data, ...             0.142857   \n",
       "2  [problem solving, group work, needed think, da...             0.750000   \n",
       "3  [machine learning, image author, multioutput r...             0.250000   \n",
       "4  [pip install, recycle bin, window version, hex...             0.500000   \n",
       "\n",
       "   percentage_medium  percentage_advanced           tag  \n",
       "0           0.000000                0.000      Beginner  \n",
       "1           0.857143                0.000  Intermediate  \n",
       "2           0.000000                0.250      Beginner  \n",
       "3           0.625000                0.125  Intermediate  \n",
       "4           0.500000                0.000  Intermediate  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Read_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## getting read time\n",
    "# according to the internet a person reads around 238 words per minute\n",
    "import math\n",
    "\n",
    "df_filtered['read_time'] = df_filtered['body'].apply(lambda x:math.ceil(len(x.split())/238))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Articles to Suggest - currently random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"if len(df_filtered) > 5:\\n    df_sample = df_filtered.sample(n=5)\\n    df_sample = df_sample['website'].apply(lambda x:x.split('/')[2]).drop_duplicates()\\n    \\n    while len(df_sample) < 5:\\n        df_sample = df_filtered.sample(n=5)\\n        df_sample = df_sample['website'].apply(lambda x:x.split('/')[2]).drop_duplicates()\\n\\ndf_sample\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying to get make sure there aren't two articles from the same webpage - works now\n",
    "\n",
    "# finish this!!\n",
    "\n",
    "'''if len(df_filtered) > 5:\n",
    "    df_sample = df_filtered.sample(n=5)\n",
    "    df_sample = df_sample['website'].apply(lambda x:x.split('/')[2]).drop_duplicates()\n",
    "    \n",
    "    while len(df_sample) < 5:\n",
    "        df_sample = df_filtered.sample(n=5)\n",
    "        df_sample = df_sample['website'].apply(lambda x:x.split('/')[2]).drop_duplicates()\n",
    "\n",
    "df_sample'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if len(df_filtered) > 5:\\n    df_filtered = df_filtered.sample(n=5)\\n\\ndf_filtered = df_filtered.reset_index(drop=True)'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choosing 5 randomly\n",
    "\n",
    "'''if len(df_filtered) > 5:\n",
    "    df_filtered = df_filtered.sample(n=5)\n",
    "\n",
    "df_filtered = df_filtered.reset_index(drop=True)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Summaries - BART Summariser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting model\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summaries(df_filtered, length):\n",
    "\n",
    "    summaries = []\n",
    "\n",
    "    for i in range(len(df_filtered['body'])):\n",
    "        document = df_filtered['body'][i]\n",
    "\n",
    "        inputs = tokenizer([document], max_length=1024, return_tensors='pt', truncation=True)\n",
    "\n",
    "        summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=length, early_stopping=True, length_penalty=2)\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    df_filtered['summary'] = summaries\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dataframe and max length of summary\n",
    "\n",
    "df_filtered = get_summaries(df_filtered, 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing last sentence from summary if not complete\n",
    "\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "summaries_complete = []\n",
    "\n",
    "for i in range(len(df_filtered['summary'])):\n",
    "    string = df_filtered['summary'][i]\n",
    "\n",
    "    sentences = sent_tokenizer.tokenize(string)\n",
    "\n",
    "    add = []\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        if sentences[i][-1] == '.':\n",
    "            add.append(sentences[i])\n",
    "\n",
    "\n",
    "    summaries_complete.append(\" \".join(s for s in add))\n",
    "\n",
    "df_filtered['summary'] = summaries_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing sentences which are the same as the title\n",
    "\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "summaries_no_title = []\n",
    "\n",
    "for i in range(len(df_filtered['summary'])):\n",
    "    string = df_filtered['summary'][i]\n",
    "\n",
    "    sentences = sent_tokenizer.tokenize(string)\n",
    "\n",
    "    add = []\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        if sentences[i][:-1] != df_filtered['title'][i]:\n",
    "            add.append(sentences[i])\n",
    "\n",
    "    summaries_no_title.append(\" \".join(s for s in add))\n",
    "\n",
    "df_filtered['summary'] = summaries_no_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing any white space from start and end of summaries\n",
    "df_filtered['summary'] = df_filtered['summary'].apply(lambda x:x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove summaries that contain code - it is usually incomplete and not helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newspaper Function - exports articles with relevant information and get's back weather information (and stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newsletter():\n",
    "\n",
    "    # make a folder to put contents\n",
    "\n",
    "    path = '/Users/Zac/Desktop/Newsletter - ' + datetime.strftime(datetime.now(), '%d-%m-%Y') + '/'\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "\n",
    "    os.mkdir(path)\n",
    "\n",
    "    with open(os.path.join('/Users/Zac/Desktop/Newsletter - ' + datetime.strftime(datetime.now(), '%d-%m-%Y') + '/','Summaries.txt'), \"w\") as f:\n",
    "        f.write(datetime.strftime(datetime.now(), '%A %d %B') + '\\n\\n')\n",
    "\n",
    "    for i in range(len(df_filtered)):\n",
    "\n",
    "        # specifify text file\n",
    "\n",
    "        with open('/Users/Zac/Desktop/Newsletter - ' + datetime.strftime(datetime.now(), '%d-%m-%Y') + '/Summaries.txt', 'a') as f:\n",
    "            f.write('Tag: ' + df_filtered['tag'][i].upper() + ' - Read Time: ' + str(df_filtered['read_time'][i]) + '\\n\\n' + df_filtered['title'][i] + '\\n\\n' + df_filtered['summary'][i] + '\\n\\n' + 'Read the full story here: ' + df_filtered['website'][i] + '\\n\\n' + '---------------' + '\\n\\n')\n",
    "\n",
    "        #¬†images\n",
    "\n",
    "        try:\n",
    "            response = requests.get(df_filtered['image'][i])\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "            img.save('/Users/Zac/Desktop/Newsletter - ' + datetime.strftime(datetime.now(), '%d-%m-%Y') + '/' + df_filtered['title'][i] +'.png')\n",
    "\n",
    "        except:\n",
    "            try:\n",
    "                img.save('/Users/Zac/Desktop/Newsletter - ' + datetime.strftime(datetime.now(), '%d-%m-%Y') + '/' + df_filtered['title'][i] +'.jpg')\n",
    "            except:\n",
    "                print('Image error on', df_filtered['title'][i])\n",
    "        continue\n",
    "\n",
    "\n",
    "    # weather\n",
    "\n",
    "    owm = OWM('b33c2d9566ba8b3c3260afc40c91d012')\n",
    "    mgr = owm.weather_manager()\n",
    "    observation = mgr.one_call(lat = 41.38879, lon = 2.15899)\n",
    "    weather = observation.forecast_daily[0]\n",
    "\n",
    "    #¬†saving weather to disc\n",
    "\n",
    "    path = '/Users/Zac/Desktop/Newsletter - ' + datetime.strftime(datetime.now(), '%d-%m-%Y') + '/Weather/'\n",
    "    \n",
    "    # make a folder to put contents\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "\n",
    "    os.mkdir(path)\n",
    "\n",
    "    with open(os.path.join('/Users/Zac/Desktop/Newsletter - ' + datetime.strftime(datetime.now(), '%d-%m-%Y') + '/Weather/' + 'weather.txt'), \"w\") as f:\n",
    "        \n",
    "        f.write('Barcelona'.upper()+ '\\n\\n')\n",
    "\n",
    "        weather_dict = {'clear sky':'Clear Skies', 'few clouds':'Partly Cloudy', 'scattered clouds':'Partly Cloudy', 'broken clouds':'Cloudy', 'shower rain':'Light Showers', 'rain':'Rain', 'thunderstorm':'Stormy', 'snow':'Snow', 'mist':'Misty'}\n",
    "\n",
    "        f.write(weather_dict[weather.detailed_status].upper() + '\\n\\n')\n",
    "    \n",
    "        # high and low temp\n",
    "        f.write('Low/High: ' + str(round(weather.temperature('celsius')['min'])) + '¬∞/' + str(round(weather.temperature('celsius')['max']))+ '¬∞\\n\\n')\n",
    "\n",
    "        # sunrise\n",
    "        timestamp = datetime.fromtimestamp(weather.srise_time)\n",
    "        f.write('Sunrise: ' + timestamp.strftime('%H:%M') + '\\n')\n",
    "\n",
    "        # sunset\n",
    "        timestamp = datetime.fromtimestamp(weather.sset_time)\n",
    "        f.write('Sunset: '+ timestamp.strftime('%H:%M'))\n",
    "\n",
    "    # getting back weather icon\n",
    "\n",
    "    try:\n",
    "\n",
    "        url = weather.weather_icon_url()\n",
    "\n",
    "        response = requests.get(url)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "\n",
    "        img.save('/Users/Zac/Desktop/Newsletter - ' + datetime.strftime(datetime.now(), '%d-%m-%Y') + '/Weather/' + 'weathericon' + '.png')\n",
    "\n",
    "    except:\n",
    "        print('image error')\n",
    "\n",
    "    # stocks\n",
    "\n",
    "    stock_weekdays = ['Tue', 'Wed', 'Thu', 'Fri']\n",
    "\n",
    "    stock_dict = {'^GSPC':'S&P 500', '^IBEX':'IBEX 35', '^IXIC':'NASDAQ', '^DJI':'DOW'}\n",
    "\n",
    "    path = '/Users/Zac/Desktop/Newsletter - ' + datetime.strftime(datetime.now(), '%d-%m-%Y') + '/Stocks/'\n",
    "    \n",
    "    # make a folder to put contents\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "\n",
    "    os.mkdir(path)\n",
    "\n",
    "    with open(os.path.join('/Users/Zac/Desktop/Newsletter - ' + datetime.strftime(datetime.now(), '%d-%m-%Y') + '/Stocks/' + 'stocks.txt'), \"w\") as f:\n",
    "        f.write('Stocks: \\n\\n')\n",
    "\n",
    "    # get daily stock change for weekdays\n",
    "\n",
    "    if datetime.strftime(datetime.now() - timedelta(1), '%a') in stock_weekdays:\n",
    "\n",
    "        for stock_ticker in stock_dict.keys():\n",
    "\n",
    "            df = pdr.get_data_yahoo(stock_ticker)\n",
    "            df = df.reset_index()\n",
    "            yesterday_close = df[df['Date'] == datetime.strftime(datetime.now() - timedelta(1), '%Y-%m-%d')]['Close'].item()\n",
    "\n",
    "            try:\n",
    "                day_before_yesterday_close = df[df['Date'] == datetime.strftime(datetime.now() - timedelta(2), '%Y-%m-%d')]['Close'].item()\n",
    "            except:\n",
    "                print('No stock data available today.')\n",
    "\n",
    "            change = yesterday_close - day_before_yesterday_close\n",
    "            percentage_change = round((change/day_before_yesterday_close)*100, 2)\n",
    "\n",
    "            with open(os.path.join('/Users/Zac/Desktop/Newsletter - ' + datetime.strftime(datetime.now(), '%d-%m-%Y') + '/Stocks/' + 'stocks.txt'), \"a\") as f:\n",
    "\n",
    "                if percentage_change > 0:\n",
    "                \n",
    "                    f.write(stock_dict[stock_ticker] + ' +' + str(percentage_change) +  '%\\n\\n')\n",
    "                \n",
    "                else:\n",
    "                    f.write(stock_dict[stock_ticker]+ ' ' +str(percentage_change) + '%\\n\\n')\n",
    "\n",
    "    # if not a weekday, get yearly change\n",
    "\n",
    "    else:\n",
    "        for stock_ticker in stock_dict.keys():\n",
    "\n",
    "            df = pdr.get_data_yahoo(stock_ticker)\n",
    "            df = df.reset_index()\n",
    "\n",
    "            start_year = df[df['Date'] == datetime.strftime(datetime(datetime.today().year, 2, 1), '%Y-%m-%d')]['Close'].item()\n",
    "            yesterday_close = df[df['Date'] == datetime.strftime(datetime.now() - timedelta(1), '%Y-%m-%d')]['Close'].item()\n",
    "\n",
    "            change = yesterday_close - start_year\n",
    "            percentage_change = round((change/start_year)*100, 2)\n",
    "\n",
    "            if percentage_change > 0:\n",
    "                    \n",
    "                f.write('Yearly Change: ' + stock_dict[stock_ticker] + ' +' + str(percentage_change) +  '%')\n",
    "                    \n",
    "            else:\n",
    "                f.write('Yearly Change: ' + stock_dict[stock_ticker]+ ' ' +str(percentage_change) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image error on Optibrium Shows Deep Learning to Successfully Predict Human Panel-based Sensory Perception of Novel Compounds Used for Flavors and Fragrances\n",
      "Image error on ‚ÄúAbove the Trend Line‚Äù ‚Äì Your Industry Rumor Central for 12/10/2021\n",
      "Image error on Heard on the Street ‚Äì 11/30/2021\n"
     ]
    }
   ],
   "source": [
    "newsletter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wishlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting words over time graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import pandas as pd\\nfrom pytrends.request import TrendReq\\n\\npytrends = TrendReq(hl='en-UK', tz=60)\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "from pytrends.request import TrendReq\n",
    "\n",
    "pytrends = TrendReq(hl='en-UK', tz=60)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use NLTK to get back top 3 words/collocations\n",
    "\n",
    "# then plot a graph of this information at the bottom of the newsletter\n",
    "\n",
    "# today's top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_filtered['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dict = {'clear sky':'Clear Skies', 'few clouds':'Partly Cloudy', 'scattered clouds':'Partly Cloudy', 'broken clouds':'Cloudy', 'shower rain':'Light Showers', 'rain':'Rain', 'thunderstorm':'Stormy', 'snow':'Snow', 'mist':'Misty'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clear Skies'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_dict['clear sky']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"list_of_bad_words = ['clear sky', 'few clouds']\\n\\nfor word in list_of_bad_words:\\n    print(weather_dict[word])\\n\\n    df['lyrics'].apply(lambda x:x.replace(weather_dict[word], ))\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''list_of_bad_words = ['clear sky', 'few clouds']\n",
    "\n",
    "for word in list_of_bad_words:\n",
    "    print(weather_dict[word])\n",
    "\n",
    "    df['lyrics'].apply(lambda x:x.replace(weather_dict[word], ))'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa2d5c2f16a8e749f1d48e9a236e4b14f675e3e0a88517126fa8ace0eecda256"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('da_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
